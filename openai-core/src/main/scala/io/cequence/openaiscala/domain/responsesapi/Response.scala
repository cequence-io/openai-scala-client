package io.cequence.openaiscala.domain.responsesapi

import java.{util => ju}
import io.cequence.openaiscala.domain.responsesapi.OutputMessageContent.OutputText
import io.cequence.openaiscala.domain.responsesapi.tools.ToolChoice
import io.cequence.openaiscala.domain.responsesapi.tools.Tool
import io.cequence.openaiscala.domain.responsesapi.tools.FunctionToolCall

/**
 * Represents a response from the model.
 *
 * @param background
 *   Whether to run the model response in the background.
 * @param conversation
 *   The conversation that this response belongs to. Input items and output items from this
 *   response are automatically added to this conversation.
 * @param createdAt
 *   Unix timestamp (in seconds) of when this Response was created.
 * @param error
 *   An error object returned when the model fails to generate a Response.
 * @param id
 *   Unique identifier for this Response.
 * @param incompleteDetails
 *   Details about why the response is incomplete.
 * @param instructions
 *   Inserts a system (or developer) message as the first item in the model's context.
 *
 * When using along with previous_response_id, the instructions from a previous response will
 * be not be carried over to the next response. This makes it simple to swap out system (or
 * developer) messages in new responses.
 * @param maxOutputTokens
 *   An upper bound for the number of tokens that can be generated for a response, including
 *   visible output tokens and reasoning tokens.
 * @param maxToolCalls
 *   The maximum number of total calls to built-in tools that can be processed in a response.
 *   This maximum number applies across all built-in tool calls, not per individual tool.
 * @param metadata
 *   Set of 16 key-value pairs that can be attached to an object.
 * @param model
 *   Model ID used to generate the response, like gpt-4o or o1.
 * @param `object`
 *   The object type of this resource - always set to response.
 * @param output
 *   An array of content items generated by the model.
 * @param outputText
 *   SDK-only convenience property that contains the aggregated text output from all
 *   output_text items in the output array, if any are present.
 * @param parallelToolCalls
 *   Whether to allow the model to run tool calls in parallel.
 * @param previousResponseId
 *   The unique ID of the previous response to the model.
 * @param prompt
 *   Reference to a prompt template and its variables.
 * @param promptCacheKey
 *   Used by OpenAI to cache responses for similar requests to optimize cache hit rates.
 *   Replaces the user field.
 * @param reasoning
 *   Configuration options for reasoning models (o-series models only).
 * @param safetyIdentifier
 *   A stable identifier used to help detect users of your application that may be violating
 *   OpenAI's usage policies. Should be a string that uniquely identifies each user (e.g.,
 *   hashed username or email).
 * @param serviceTier
 *   Specifies the processing type used for serving the request. One of auto, default, flex, or
 *   priority. When not set, the default behavior is auto.
 * @param status
 *   The status of the response generation. One of completed, failed, in_progress, or
 *   incomplete.
 * @param temperature
 *   What sampling temperature to use, between 0 and 2.
 * @param text
 *   Configuration options for a text response from the model.
 * @param toolChoice
 *   How the model should select which tool (or tools) to use when generating a response.
 * @param tools
 *   An array of tools the model may call while generating a response.
 * @param topLogprobs
 *   An integer between 0 and 20 specifying the number of most likely tokens to return at each
 *   token position, each with an associated log probability.
 * @param topP
 *   An alternative to sampling with temperature, called nucleus sampling.
 * @param truncation
 *   The truncation strategy to use for the model response. auto: If the context of this
 *   response and previous ones exceeds the model's context window size, the model will
 *   truncate the response to fit the context window by dropping input items in the middle of
 *   the conversation. disabled (default): If a model response will exceed the context window
 *   size for a model, the request will fail with a 400 error.
 * @param usage
 *   Represents token usage details including input tokens, output tokens, a breakdown of
 *   output tokens, and the total tokens used.
 * @param user
 *   A unique identifier representing your end-user, which can help OpenAI to monitor and
 *   detect abuse.
 */
final case class Response(
  background: Option[Boolean] = None,
  conversation: Option[Conversation] = None,
  createdAt: ju.Date,
  error: Option[ResponseError] = None,
  id: String,
  incompleteDetails: Option[IncompleteDetails] = None,
  instructions: Option[Inputs] = None,
  maxOutputTokens: Option[Int] = None,
  maxToolCalls: Option[Int] = None,
  metadata: Option[Map[String, String]] = None,
  model: String,
  `object`: String = "response",
  output: Seq[Output] = Nil,
  parallelToolCalls: Boolean,
  previousResponseId: Option[String] = None,
  prompt: Option[Prompt] = None,
  promptCacheKey: Option[String] = None,
  reasoning: Option[ReasoningConfig] = None,
  safetyIdentifier: Option[String] = None,
  serviceTier: Option[String] = None,
  status: ModelStatus,
  temperature: Option[Double] = None,
  text: TextResponseConfig,
  toolChoice: Option[ToolChoice] = None,
  tools: Seq[Tool] = Nil,
  topLogprobs: Option[Int] = None,
  topP: Option[Double] = None,
  truncation: Option[TruncationStrategy] = None,
  usage: Option[UsageInfo] = None,
  user: Option[String] = None
) {

  /**
   * Shortcut that returns the output message contents from the response.
   *
   * @return
   */
  def outputMessageContents: Seq[OutputMessageContent] =
    output.collect { case output: Message.OutputContent => output.content }.flatten

  /**
   * Shortcut that returns the function calls from the response.
   *
   * @return
   */
  def outputFunctionCalls: Seq[FunctionToolCall] =
    output.collect { case output: FunctionToolCall => output }

  /**
   * SDK-only convenience property that contains the aggregated text output from all
   * output_text items in the output array, if any are present. Supported in the Python,
   * JavaScript, and Scala SDKs.
   *
   * @return
   */
  def outputText: Option[String] = {
    val texts = outputMessageContents.collect { case e: OutputText => e.text }

    if (texts.isEmpty)
      None
    else
      Some(texts.mkString("\n"))
  }
}

/**
 * Represents details about why the response is incomplete.
 *
 * @param reason
 *   The reason why the response is incomplete.
 */
case class IncompleteDetails(
  reason: String
)

/**
 * Represents an error that occurred during response generation.
 *
 * @param code
 *   The error code for the response.
 * @param message
 *   A human-readable description of the error.
 */
case class ResponseError(
  code: String,
  message: String
)

/**
 * Represents a conversation that a response belongs to.
 *
 * @param id
 *   The unique ID of the conversation.
 */
case class Conversation(
  id: String
)
