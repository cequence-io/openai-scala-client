package io.cequence.openaiscala.anthropic.domain.settings

import io.cequence.openaiscala.anthropic.domain.OutputFormat
import io.cequence.openaiscala.anthropic.domain.skills.Container
import io.cequence.openaiscala.anthropic.domain.tools.{
  MCPServerURLDefinition,
  Tool,
  ToolChoice
}
import io.cequence.wsclient.domain.EnumValue

final case class AnthropicCreateMessageSettings(
  // The model that will complete your prompt.
  // See [[models|https://docs.anthropic.com/claude/docs/models-overview]] for additional details and options.
  model: String,

  // The maximum number of tokens to generate before stopping.
  // Note that our models may stop before reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate.
  // Different models have different maximum values for this parameter. See [[models|https://docs.anthropic.com/claude/docs/models-overview]] for details.
  max_tokens: Int,

  // An object describing metadata about the request.
  // user_id - An external identifier for the user who is associated with the request.
  // This should be a uuid, hash value, or other opaque identifier. Anthropic may use this id to help detect abuse. Do not include any identifying information such as name, email address, or phone number.
  metadata: Map[String, String] = Map.empty,

  // Custom text sequences that will cause the model to stop generating.
  // Our models will normally stop when they have naturally completed their turn, which will result in a response stop_reason of "end_turn".
  // If you want the model to stop generating when it encounters custom strings of text, you can use the stop_sequences parameter. If the model encounters one of the custom sequences, the response stop_reason value will be "stop_sequence" and the response stop_sequence value will contain the matched stop sequence.
  stop_sequences: Seq[String] = Seq.empty,

  // Amount of randomness injected into the response.
  // Defaults to 1.0. Ranges from 0.0 to 1.0. Use temperature closer to 0.0 for analytical / multiple choice, and closer to 1.0 for creative and generative tasks.
  // Note that even with temperature of 0.0, the results will not be fully deterministic.
  temperature: Option[Double] = None,

  // Use nucleus sampling.
  // In nucleus sampling, we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order and cut it off once it reaches a particular probability specified by top_p. You should either alter temperature or top_p, but not both.
  // Recommended for advanced use cases only. You usually only need to use temperature.
  top_p: Option[Double] = None,

  // Only sample from the top K options for each subsequent token.
  // Used to remove "long tail" low probability responses. Learn more technical details here.
  // Recommended for advanced use cases only. You usually only need to use temperature.
  top_k: Option[Int] = None,

  // Configuration for enabling Claude's extended thinking.
  // When enabled, responses include thinking content blocks showing Claude's thinking process before the final answer.
  // Requires a minimum budget of 1,024 tokens and counts towards your max_tokens limit.
  thinking: Option[ThinkingSettings] = None,

  // Container identifier for reuse across requests.
  // Container parameters with skills to be loaded.
  // Maximum 8 skills can be loaded in a container.
  container: Option[Container] = None,

  // Definitions of tools that the model may use.
  // If you include tools in your API request, the model may return tool_use content blocks that represent the model's use of those tools.
  // You can then run those tools using the tool input generated by the model and then optionally return results back to the model using tool_result content blocks.
  tools: Seq[Tool] = Nil,

  // How the model should use the provided tools. The model can use a specific tool, any available tool, decide by itself, or not use tools at all.
  tool_choice: Option[ToolChoice] = None,

  // MCP servers to be utilized in this request. Maximum length: 20
  mcp_servers: Seq[MCPServerURLDefinition] = Nil,

  // Defines the format for structured output from Claude.
  // When specified, Claude will generate responses that conform to the provided JSON schema.
  // Structured outputs are currently available as a public beta feature in the Claude API for Claude Sonnet 4.5 and Claude Opus 4.1.
  output_format: Option[OutputFormat] = None
)

final case class ThinkingSettings(
  // Determines how many tokens Claude can use for its internal reasoning process. Larger budgets can enable more thorough analysis for complex problems, improving response quality.
  // Must be â‰¥1024 and less than max_tokens.
  // See extended thinking for details.
  // Required range: x > 1024
  budget_tokens: Int,

  // Type of thinking process.
  // Available options: enabled
  `type`: ThinkingType = ThinkingType.enabled
)

sealed trait ThinkingType extends EnumValue

object ThinkingType {
  case object enabled extends ThinkingType

  def values: Seq[ThinkingType] = Seq(enabled)
}
