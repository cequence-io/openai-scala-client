package io.cequence.openaiscala.anthropic.domain.settings

import io.cequence.openaiscala.anthropic.domain.OutputFormat
import io.cequence.openaiscala.anthropic.domain.skills.Container
import io.cequence.openaiscala.anthropic.domain.tools.{
  MCPServerURLDefinition,
  Tool,
  ToolChoice
}
import io.cequence.wsclient.domain.EnumValue

final case class AnthropicCreateMessageSettings(
  // The model that will complete your prompt.
  // See [[models|https://docs.anthropic.com/claude/docs/models-overview]] for additional details and options.
  model: String,

  // The maximum number of tokens to generate before stopping.
  // Note that our models may stop before reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate.
  // Different models have different maximum values for this parameter. See [[models|https://docs.anthropic.com/claude/docs/models-overview]] for details.
  max_tokens: Int,

  // An object describing metadata about the request.
  // user_id - An external identifier for the user who is associated with the request.
  // This should be a uuid, hash value, or other opaque identifier. Anthropic may use this id to help detect abuse. Do not include any identifying information such as name, email address, or phone number.
  metadata: Map[String, String] = Map.empty,

  // Custom text sequences that will cause the model to stop generating.
  // Our models will normally stop when they have naturally completed their turn, which will result in a response stop_reason of "end_turn".
  // If you want the model to stop generating when it encounters custom strings of text, you can use the stop_sequences parameter. If the model encounters one of the custom sequences, the response stop_reason value will be "stop_sequence" and the response stop_sequence value will contain the matched stop sequence.
  stop_sequences: Seq[String] = Seq.empty,

  // Amount of randomness injected into the response.
  // Defaults to 1.0. Ranges from 0.0 to 1.0. Use temperature closer to 0.0 for analytical / multiple choice, and closer to 1.0 for creative and generative tasks.
  // Note that even with temperature of 0.0, the results will not be fully deterministic.
  temperature: Option[Double] = None,

  // Use nucleus sampling.
  // In nucleus sampling, we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order and cut it off once it reaches a particular probability specified by top_p. You should either alter temperature or top_p, but not both.
  // Recommended for advanced use cases only. You usually only need to use temperature.
  top_p: Option[Double] = None,

  // Only sample from the top K options for each subsequent token.
  // Used to remove "long tail" low probability responses. Learn more technical details here.
  // Recommended for advanced use cases only. You usually only need to use temperature.
  top_k: Option[Int] = None,

  // Configuration for enabling Claude's extended thinking.
  // When enabled, responses include thinking content blocks showing Claude's thinking process before the final answer.
  // Requires a minimum budget of 1,024 tokens and counts towards your max_tokens limit.
  thinking: Option[ThinkingSettings] = None,

  // Container identifier for reuse across requests.
  // Container parameters with skills to be loaded.
  // Maximum 8 skills can be loaded in a container.
  container: Option[Container] = None,

  // Definitions of tools that the model may use.
  // If you include tools in your API request, the model may return tool_use content blocks that represent the model's use of those tools.
  // You can then run those tools using the tool input generated by the model and then optionally return results back to the model using tool_result content blocks.
  tools: Seq[Tool] = Nil,

  // How the model should use the provided tools. The model can use a specific tool, any available tool, decide by itself, or not use tools at all.
  tool_choice: Option[ToolChoice] = None,

  // MCP servers to be utilized in this request. Maximum length: 20
  mcp_servers: Seq[MCPServerURLDefinition] = Nil,

  // Defines the format for structured output from Claude.
  // When specified, Claude will generate responses that conform to the provided JSON schema.
  // Structured outputs are currently available as a public beta feature in the Claude API for Claude Sonnet 4.5 and Claude Opus 4.1.
  output_format: Option[OutputFormat] = None,

  // Configuration options for the model's output. Controls aspects like how much effort the model puts into its response.
  output_config: Option[OutputConfig] = None,

  // Fast mode delivers significantly faster output token generation for Opus models.
  // Fast mode is up to 2.5x as fast at premium pricing ($30/$150 per MTok).
  // This is the same model running with faster inference (no change to intelligence or capabilities).
  // Requires the beta header "fast-mode-2026-02-01".
  speed: Option[Speed] = None
)

// Configuration options for the model's output.
final case class OutputConfig(
  // Controls how much effort the model puts into its response.
  effort: Option[OutputEffort] = None
)

sealed trait OutputEffort extends EnumValue

// The effort parameter is supported by Claude Opus 4.6 and Claude Opus 4.5.
object OutputEffort {
  case object low extends OutputEffort
  case object medium extends OutputEffort
  case object high extends OutputEffort
  // Absolute maximum capability with no constraints on token spending.
  // Opus 4.6 only - requests using max on other models will return an error.
  case object max extends OutputEffort

  def values: Seq[OutputEffort] = Seq(low, medium, high, max)
}

final case class ThinkingSettings(
  // Type of thinking process.
  // Available options: enabled, adaptive
  // For Opus 4.6+, use adaptive (recommended). Claude dynamically decides when and how much to think.
  // Note: "enabled" and budget_tokens are deprecated on Opus 4.6 but remain functional.
  `type`: ThinkingType = ThinkingType.enabled,

  // Determines how many tokens Claude can use for its internal reasoning process.
  // Larger budgets can enable more thorough analysis for complex problems.
  // Must be â‰¥1024 and less than max_tokens.
  // Only required for type=enabled. Not used with type=adaptive.
  // Deprecated on Opus 4.6 - use adaptive thinking with effort parameter instead.
  budget_tokens: Option[Int] = None
)

object ThinkingSettings {
  // Convenience constructor for adaptive thinking (recommended for Opus 4.6+)
  def adaptive: ThinkingSettings = ThinkingSettings(`type` = ThinkingType.adaptive)

  // Convenience constructor for enabled thinking with budget (legacy)
  def enabled(budgetTokens: Int): ThinkingSettings =
    ThinkingSettings(`type` = ThinkingType.enabled, budget_tokens = Some(budgetTokens))
}

sealed trait ThinkingType extends EnumValue

object ThinkingType {
  case object enabled extends ThinkingType
  case object adaptive extends ThinkingType

  def values: Seq[ThinkingType] = Seq(enabled, adaptive)
}

sealed trait Speed extends EnumValue

object Speed {
  case object fast extends Speed

  def values: Seq[Speed] = Seq(fast)
}
